"""
master_loader.py - ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ CSV to JSON è‡ªå‹•å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx - å•†å“ãƒã‚¹ã‚¿.csvï¼‰ã‚’èª­ã¿è¾¼ã¿ã€
æ§‹é€ åŒ–ã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆproduction_master.jsonï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
"""

import pandas as pd
import json
import os
import glob
import logging

logger = logging.getLogger(__name__)

# --- ãƒ‘ã‚¹è¨­å®š ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, 'data')
CSV_PATH = os.path.join(DATA_DIR, 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx - å•†å“ãƒã‚¹ã‚¿.csv')
JSON_PATH = os.path.join(DATA_DIR, 'production_master.json')
HISTORY_PATH = os.path.join(DATA_DIR, 'history_summary.json')


def get_val(row, col, default=0):
    """æ•°å€¤ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å®‰å…¨ãªå–å¾—ã€‚NaNãƒ»ç©ºæ–‡å­—ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™ã€‚"""
    val = row.get(col)
    if pd.isna(val) or val == '':
        return default
    return val


def get_str(row, col, default=""):
    """æ–‡å­—åˆ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å®‰å…¨ãªå–å¾—ã€‚NaNãƒ»ç©ºæ–‡å­—ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™ã€‚"""
    val = row.get(col)
    if pd.isna(val) or val == '':
        return default
    return str(val)


def find_latest_csv(directory):
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã™ã‚‹ã€‚
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼šãƒ¡ã‚¤ãƒ³ã®CSVãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«ä½¿ç”¨ã€‚

    Returns:
        str or None: æœ€æ–°CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneã€‚
    """
    if not os.path.isdir(directory):
        return None

    csv_files = glob.glob(os.path.join(directory, '*.csv'))
    # confirmed_log.csv ã¯é™¤å¤–
    csv_files = [f for f in csv_files if 'confirmed_log' not in os.path.basename(f).lower()]

    if not csv_files:
        return None

    # æ›´æ–°æ—¥æ™‚ãŒæœ€æ–°ã®ã‚‚ã®ã‚’è¿”ã™
    latest = max(csv_files, key=os.path.getmtime)
    logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€æ–°CSVæ¤œå‡º â†’ {latest}")
    return latest


def convert_dataframe_to_json(df, force=False, excel_bytes=None):
    """
    DataFrameã‚’å—ã‘å–ã‚Šã€æ§‹é€ åŒ–ã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    excel_bytes ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã€è‡ªå‹•çš„ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆç®—ã‚‚è¡Œã†ã€‚

    Args:
        df (pd.DataFrame): ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®DataFrame
        force (bool): True ã®å ´åˆã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒã‚§ãƒƒã‚¯ã‚’ç„¡è¦–ã—ã¦ä¿å­˜ã™ã‚‹ï¼ˆDFã®å ´åˆã¯å¸¸ã«ä¿å­˜æ¨å¥¨ï¼‰
        excel_bytes (bytes): ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±å–å¾—ç”¨ï¼‰

    Returns:
        list: å¤‰æ›ã•ã‚ŒãŸãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã€‚
    """
    master_list = []
    for index, row in df.iterrows():
        if pd.isna(row.get('ID')):
            continue

        item = {
            "id": get_str(row, 'ID'),
            "category": get_str(row, 'ã‚«ãƒ†ã‚´ãƒª'),
            "name": get_str(row, 'å•†å“å'),
            "part": get_str(row, 'éƒ¨ä½'),
            "price": int(get_val(row, 'å˜ä¾¡1')),
            "current_stock": int(get_val(row, 'åœ¨åº«æ•°')),
            "requirements": {
                "yield": float(get_val(row, 'å–æ•°', 1)),
                "material_type": get_str(row, 'ææ–™ç¨®åˆ¥'),
                "nc_machine_type": get_str(row, 'NCãƒã‚·ãƒ³', 'Both')
            },
            "process": {
                "prep": {
                    "setup_min": float(get_val(row, 'ç”Ÿåœ°_å›ºå®š')),
                    "unit_min": float(get_val(row, 'ç”Ÿåœ°_å˜ä½“')),
                    "drying_hr": float(get_val(row, 'ç”Ÿåœ°ä¹¾ç‡¥h'))
                },
                "nc": {
                    "front_rough_min": float(get_val(row, 'NCè¡¨_ç²—åˆ†')),
                    "front_finish_min": float(get_val(row, 'NCè¡¨_ä»•åˆ†')),
                    "back_rough_min": float(get_val(row, 'NCè£_ç²—åˆ†')),
                    "back_finish_min": float(get_val(row, 'NCè£_ä»•åˆ†'))
                },
                "assembly": {
                    "cut_off_min": float(get_val(row, 'åˆ‡é›¢åˆ†')),
                    "bonding_min": float(get_val(row, 'çµ„ä»˜æ¥ç€åˆ†')),
                    "drying_hr": float(get_val(row, 'çµ„ä»˜ä¹¾ç‡¥h'))
                },
                "manual": {
                    "fitting_min": float(get_val(row, 'åµŒåˆèª¿æ•´åˆ†')),
                    "machine_work_min": float(get_val(row, 'æ©Ÿæ¢°åŠ å·¥åˆ†')),
                    "sanding_min": float(get_val(row, 'ç ”ç£¨æ‰‹åŠ åˆ†')),
                    "assembly_min": float(get_val(row, 'çµ„ç«‹ç‰å…¥åˆ†'))
                }
            }
        }
        master_list.append(item)

    # --- Phase 3: ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆç®— (å†…éƒ¨çµåˆ) ---
    if excel_bytes:
        logger.info("Excelãƒã‚¤ãƒŠãƒªãŒæ¸¡ã•ã‚ŒãŸãŸã‚ã€ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆç®—ã—ã¾ã™ã€‚")
        # merge_event_targets ã¯ãƒªã‚¹ãƒˆã‚’æ›¸ãæ›ãˆã¦è¿”ã™
        # NOTE: merge_event_targets å†…éƒ¨ã§ã®JSONä¿å­˜ã¯é‡è¤‡ã«ãªã‚‹ãŒã€
        # ã“ã“ã§å‘¼ã¶ã“ã¨ã§ç¢ºå®Ÿã«åæ˜ ã•ã›ã‚‹ã€‚
        # ãŸã ã—ã€merge_event_targets ã‹ã‚‰JSONä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã™ã‚‹ã®ãŒç¶ºéº—ã ãŒã€
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œç·Šæ€¥å‘½ä»¤ã€ã‚’ç¢ºå®Ÿã«æº€ãŸã™ãŸã‚ã€ã“ã“ã§ã®å‘¼ã³å‡ºã—ã‚’å„ªå…ˆã™ã‚‹ã€‚
        # master_list ã¯å‚ç…§æ¸¡ã—ã•ã‚Œã‚‹ãŸã‚å¤‰æ•°ã¯æ›´æ–°ã•ã‚Œã‚‹ã€‚
        master_list = merge_event_targets(master_list, excel_bytes)
        
    # --- å®‰å…¨è£…ç½®: ãƒ‡ãƒ¼ã‚¿é‡ãƒã‚§ãƒƒã‚¯ ---
    # æ—¢å­˜ã®JSONãŒã‚ã‚Šã€ã‹ã¤æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒæ¥µç«¯ã«å°‘ãªã„ï¼ˆä¾‹: 10ä»¶æœªæº€ï¼‰å ´åˆã¯
    # èª¤ã£ã¦ä¸Šæ›¸ãã—ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç­‰ã«ã‚ˆã‚‹äº‹æ•…é˜²æ­¢ï¼‰
    if os.path.exists(JSON_PATH) and len(master_list) < 10:
        try:
            with open(JSON_PATH, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
            if len(old_data) > 20:
                logger.warning(f"âš ï¸ Data Safety Guard: New data has {len(master_list)} items, but old data had {len(old_data)}. Skipping overwrite.")
                print(f"âš ï¸ Data Safety Guard: Skipping overwrite to protect data. (New: {len(master_list)}, Old: {len(old_data)})")
                return old_data
        except Exception:
            pass # èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯ç„¡è¦–ã—ã¦ä¸Šæ›¸ã

    # --- JSONæ›¸ãå‡ºã— ---
    # merge_event_targets ã§ã‚‚ä¿å­˜ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒã€
    # converté–¢æ•°ã®è²¬å‹™ã¨ã—ã¦ã“ã“ã§ã‚‚ä¿å­˜ã™ã‚‹ (æœ€çµ‚çš„ãªæ•´åˆæ€§ã®ãŸã‚)
    try:
        os.makedirs(os.path.dirname(JSON_PATH), exist_ok=True)
        with open(JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(master_list, f, indent=2, ensure_ascii=False)
        msg = f"SUCCESS: production_master.json has been created at {JSON_PATH} ({len(master_list)} items)"
        logger.info(msg)
        print(msg) # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å¼·åˆ¶å‡ºåŠ›
    except Exception as e:
        logger.error(f"JSONæ›¸ãå‡ºã—å¤±æ•—: {e}")
        print(f"ERROR: Failed to create production_master.json: {e}")
        return master_list

    return master_list


# --- Driveé€£æºç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from logic import drive_utils
    from logic.drive_utils import upload_to_drive, HISTORY_SUMMARY_DRIVE_ID
except ImportError:
    try:
        import drive_utils
        from drive_utils import upload_to_drive, HISTORY_SUMMARY_DRIVE_ID
    except ImportError:
        drive_utils = None
        upload_to_drive = None
        HISTORY_SUMMARY_DRIVE_ID = None


def sync_from_drive():
    """
    Google Driveã‹ã‚‰ã€Œãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsxã€ã‚’å–å¾—ã—ã€master JSONã‚’æ›´æ–°ã™ã‚‹ã€‚
    
    Returns:
        list: æ›´æ–°å¾Œã®ãƒã‚¹ã‚¿ãƒªã‚¹ãƒˆ
    """
    if not drive_utils:
        logger.error("drive_utilsãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€DriveåŒæœŸã§ãã¾ã›ã‚“ã€‚")
        return []

    print("--- Google Drive Sync Start ---")
    logger.info("Google Driveèªè¨¼é–‹å§‹...")
    
    try:
        service = drive_utils.authenticate()
        if not service:
            logger.error("Google Driveèªè¨¼å¤±æ•—")
            return []

        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        file_meta = drive_utils.find_file(service, "ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
        if not file_meta:
            logger.error("Driveä¸Šã« 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼' ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []

        file_id = file_meta['id']
        file_name = file_meta['name']
        mime_type = file_meta['mimeType']
        
        print(f"File Found: {file_name} ({file_id})")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        stream = drive_utils.download_content(service, file_id, mime_type)
        if not stream:
            logger.error("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            return []
            
        # Excelã¨ã—ã¦èª­ã¿è¾¼ã¿
        try:
            excel_bytes = stream.getvalue()
            df = pd.read_excel(stream, sheet_name="å•†å“ãƒã‚¹ã‚¿")
            print(f"Excel Loaded: {len(df)} rows")
        except Exception as e:
            logger.error(f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

        # JSONå¤‰æ› & ä¿å­˜ (ã‚¤ãƒ™ãƒ³ãƒˆåˆç®—å«ã‚€)
        # convert_dataframe_to_json ã¯å†…éƒ¨ã§ production_master.json ã‚’ä¿å­˜ã™ã‚‹
        master_list = convert_dataframe_to_json(df, force=True, excel_bytes=excel_bytes)
        
        print("--- Google Drive Sync Completed ---")
        return master_list

    except Exception as e:
        logger.error(f"DriveåŒæœŸäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []


def convert_csv_to_json(force=False):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰JSONã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§Google Driveã‹ã‚‰ã®åŒæœŸã‚’è©¦ã¿ã‚‹ã€‚
    å¤±æ•—ã—ãŸå ´åˆã€ãƒ­ãƒ¼ã‚«ãƒ«CSV (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    """
    # 1. DriveåŒæœŸã‚’è©¦è¡Œ
    print("Attempting Google Drive Sync...")
    master_list = sync_from_drive()
    
    if master_list:
        return master_list
    
    print("Drive Sync Failed. Falling back to local CSV...")
    
    # 2. ãƒ­ãƒ¼ã‚«ãƒ«CSVãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (æ—§ãƒ­ã‚¸ãƒƒã‚¯)
    # --- CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®š ---
    csv_path = CSV_PATH
    if not os.path.exists(csv_path):
        logger.warning(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆCSVä¸åœ¨: {csv_path}")
        csv_path = find_latest_csv(DATA_DIR)
        if csv_path is None:
            logger.error(f"data/ ãƒ•ã‚©ãƒ«ãƒ€å†…ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DATA_DIR}")
            return []

    # --- CSVèª­ã¿è¾¼ã¿ ---
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"CSVèª­ã¿è¾¼ã¿æˆåŠŸ: {csv_path} ({len(df)} è¡Œ)")
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(csv_path, encoding='cp932')
            logger.info(f"CSVèª­ã¿è¾¼ã¿æˆåŠŸ (cp932): {csv_path} ({len(df)} è¡Œ)")
        except Exception as e:
            logger.error(f"CSVèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return load_master_json()
    except Exception as e:
        logger.error(f"CSVèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return load_master_json()

    return convert_dataframe_to_json(df, force=True)


def load_master_json():
    """
    æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è¿”ã™ã€‚

    Returns:
        list: ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã€‚ãƒ•ã‚¡ã‚¤ãƒ«ä¸åœ¨ãƒ»ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã€‚
    """
    if not os.path.exists(JSON_PATH):
        return []
    try:
        with open(JSON_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"JSONèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return []



def _import_initial_from_note(xls, note_text, history_path):
    """
    å‚™è€ƒãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹: "ã‚¯ãƒªãƒ2512 AKåˆ—"ï¼‰ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€
    æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ãƒˆã®æŒ‡å®šåˆ—ã‹ã‚‰åˆæœŸåœ¨åº«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Šã€
    history_summary.json ã« type="initial" ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ã€‚
    
    Args:
        xls: pd.ExcelFile ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        note_text (str): å‚™è€ƒãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹: "ã‚¯ãƒªãƒ2512 AKåˆ—"ï¼‰
        history_path (str): history_summary.json ã®ãƒ‘ã‚¹
    """
    import re
    
    if not note_text:
        return
    
    # ãƒ‘ãƒ¼ã‚¹: "ã‚¯ãƒªãƒ2512 AKåˆ—" -> sheet_name="ã‚¯ãƒªãƒ2512", col_letter="AK"
    # ãƒ‘ã‚¿ãƒ¼ãƒ³: ã‚·ãƒ¼ãƒˆå + åˆ—å(ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ) + "åˆ—"
    match = re.match(r'(.+?)\s+([A-Za-z]+)åˆ—', note_text)
    if not match:
        logger.warning(f"å‚™è€ƒãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‘ãƒ¼ã‚¹å¤±æ•—: '{note_text}' (æœŸå¾…å½¢å¼: 'ã‚·ãƒ¼ãƒˆå åˆ—ååˆ—')")
        return
    
    target_sheet = match.group(1).strip()
    col_letter = match.group(2).strip().upper()
    
    # åˆ—æ–‡å­—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ› (A=0, B=1, ..., AK=36, AL=37)
    col_idx = 0
    for ch in col_letter:
        col_idx = col_idx * 26 + (ord(ch) - ord('A') + 1)
    col_idx -= 1  # 0-indexed
    
    logger.info(f"å‚™è€ƒã‹ã‚‰åˆæœŸåœ¨åº«å‚ç…§å…ˆã‚’ç‰¹å®š: ã‚·ãƒ¼ãƒˆ='{target_sheet}', åˆ—={col_letter}(idx={col_idx})")
    
    if target_sheet not in xls.sheet_names:
        logger.warning(f"åˆæœŸåœ¨åº«å‚ç…§å…ˆã‚·ãƒ¼ãƒˆ '{target_sheet}' ãŒExcelå†…ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return
    
    # æ—¢ã« initial ã‚¨ãƒ³ãƒˆãƒªãŒæ­£ã—ãå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if os.path.exists(history_path):
        try:
            with open(history_path, 'r', encoding='utf-8') as f:
                hist = json.load(f)
            for h in hist:
                if h.get('type') == 'initial' and h.get('details') and len(h.get('details', {})) > 2:
                    logger.info("åˆæœŸåœ¨åº«ã¯æ—¢ã«æ­£ã—ãç™»éŒ²æ¸ˆã¿ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    return
        except Exception:
            pass
    
    try:
        df_raw = pd.read_excel(xls, sheet_name=target_sheet, header=None)
        
        # IDåˆ—ã‚’æ¢ã™ (é€šå¸¸ Cåˆ—=idx 2)
        header_row_idx = -1
        id_col_idx = -1
        
        for r_idx in range(min(10, len(df_raw))):
            row_vals = [str(x).strip().upper() for x in df_raw.iloc[r_idx].values]
            if 'ID' in row_vals:
                header_row_idx = r_idx
                id_col_idx = row_vals.index('ID')
                break
        
        if header_row_idx == -1:
            logger.warning(f"ã‚·ãƒ¼ãƒˆ '{target_sheet}' ã‹ã‚‰ 'ID' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        initial_details = {}
        total_count = 0
        
        for i in range(header_row_idx + 1, len(df_raw)):
            row = df_raw.iloc[i]
            
            # IDå–å¾—
            if id_col_idx >= len(row):
                continue
            raw_id = row.iloc[id_col_idx]
            if pd.isna(raw_id):
                continue
            clean_id = str(raw_id).strip()
            if not clean_id:
                continue
            
            # æŒ‡å®šåˆ—ã‹ã‚‰ã‚«ã‚¦ãƒ³ãƒˆå–å¾—
            count = 0
            if col_idx < len(row):
                try:
                    val = row.iloc[col_idx]
                    count = int(float(val)) if pd.notna(val) else 0
                except (ValueError, TypeError):
                    count = 0
            
            if count > 0:
                initial_details[clean_id] = {"count": count, "target": 0}
                total_count += count
        
        if not initial_details:
            logger.warning(f"åˆæœŸåœ¨åº«ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ˆã‚·ãƒ¼ãƒˆ '{target_sheet}' {col_letter}åˆ—ï¼‰ã€‚")
            return
        
        # history_summary.json ã« initial ã‚¨ãƒ³ãƒˆãƒªã‚’æ›´æ–°/è¿½åŠ 
        from datetime import datetime
        new_entry = {
            "type": "initial",
            "date": "2025-12-14",
            "timestamp": datetime(2025, 12, 14, 23, 59, 59).isoformat(),
            "total_current": total_count,
            "total_target": 0,
            "details": initial_details,
            "source_note": note_text
        }
        
        history_list = []
        if os.path.exists(history_path):
            try:
                with open(history_path, 'r', encoding='utf-8') as f:
                    history_list = json.load(f)
            except Exception:
                history_list = []
        
        # æ—¢å­˜ã® initial ã‚’å‰Šé™¤ã—ã¦å…¥ã‚Œæ›¿ãˆ
        history_list = [h for h in history_list if h.get('type') != 'initial']
        history_list.insert(0, new_entry)
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history_list, f, indent=2, ensure_ascii=False)
        
        # --- Phase 1: DriveåŒæœŸ ---
        if upload_to_drive and HISTORY_SUMMARY_DRIVE_ID:
            _ok, _msg = upload_to_drive(history_path, HISTORY_SUMMARY_DRIVE_ID)
            logger.info(f"[DriveåŒæœŸ] åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾Œ: {_msg}")
        
        logger.info(f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†ï¼ˆå‚™è€ƒå‚ç…§ï¼‰: {len(initial_details)} ä»¶, ç·æ•° {total_count}")
        print(f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {len(initial_details)} ä»¶ (from {target_sheet} {col_letter}åˆ—)")
        
    except Exception as e:
        logger.error(f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


def merge_event_targets(master_list, excel_bytes, _unused_sheet_name=None):
    """
    ã€æ–°ãƒ­ã‚¸ãƒƒã‚¯ã€‘ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿ã§ã€Œã‚¢ã‚¯ãƒ†ã‚£ãƒ–/è¡¨ç¤ºã€ã¨ãªã£ã¦ã„ã‚‹å…¨ã‚¤ãƒ™ãƒ³ãƒˆã®ç›®æ¨™ã‚’åˆç®—ã—ã¦çµ±åˆã™ã‚‹ã€‚
    æ—§å¼•æ•° `sheet_name` ã¯äº’æ›æ€§ã®ãŸã‚æ®‹ã™ãŒä½¿ç”¨ã—ãªã„ï¼ˆ_unused_sheet_nameï¼‰ã€‚
    
    Args:
        master_list (list): convert_dataframe_to_json ã®å‡ºåŠ›
        excel_bytes (bytes): ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
    
    Returns:
        list: target_quantity(åˆç®—å€¤) ã¨ event_data(è©³ç´°) ãŒè¿½åŠ ã•ã‚ŒãŸ master_list
    """
    import io as _io
    
    if not excel_bytes:
        return master_list
    
    try:
        xls = pd.ExcelFile(_io.BytesIO(excel_bytes))
    except Exception as e:
        logger.error(f"Excelãƒã‚¤ãƒŠãƒªèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return master_list

    # 1. ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿ã‹ã‚‰ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚·ãƒ¼ãƒˆã‚’ç‰¹å®š
    target_sheets = []
    display_events = [] # Zeusç›£è¦–ç”¨

    if 'ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿' in xls.sheet_names:
        try:
            master_sheet = pd.read_excel(xls, sheet_name='ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿')
            
            # ã‚«ãƒ©ãƒ ç‰¹å®š
            col_map = {
                'active': None,
                'display': None,
                'sheet': None,
                'name': None,
                'deadline': None,
                'date': None,
                'venue': None,
                'booth': None,
                'loadin': None,
                'note': None  # å‚™è€ƒåˆ—ï¼ˆåˆæœŸåœ¨åº«å‚ç…§å…ˆï¼‰
            }
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼æ¢ç´¢
            for col in master_sheet.columns:
                c_str = str(col).strip()
                if 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' in c_str or 'Active' in c_str or 'é€²è»' in c_str: col_map['active'] = col
                if 'è¡¨ç¤º' in c_str or 'Display' in c_str or 'ç›£è¦–' in c_str: col_map['display'] = col
                if 'ã‚·ãƒ¼ãƒˆ' in c_str or 'å¯¾è±¡' in c_str: col_map['sheet'] = col
                if 'ã‚¤ãƒ™ãƒ³ãƒˆå' in c_str: col_map['name'] = col
                if 'ç· åˆ‡' in c_str or 'Deadline' in c_str: col_map['deadline'] = col
                if 'é–‹å‚¬' in c_str or 'Date' in c_str: col_map['date'] = col
                if 'ä¼šå ´' in c_str or 'Venue' in c_str: col_map['venue'] = col
                if 'ãƒ–ãƒ¼ã‚¹' in c_str or 'Booth' in c_str: col_map['booth'] = col
                if 'æ¬å…¥' in c_str or 'LoadIn' in c_str: col_map['loadin'] = col
                if 'å‚™è€ƒ' in c_str or 'Note' in c_str or 'note' in c_str: col_map['note'] = col

            # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯ï¼ˆã‚·ãƒ¼ãƒˆåã¯å¿…é ˆï¼‰
            if col_map['sheet']:
                for _, row in master_sheet.iterrows():
                    # å€¤å–å¾—ãƒ˜ãƒ«ãƒ‘ãƒ¼
                    def _get_val(c_key):
                        if not col_map[c_key]: return None
                        val = row.get(col_map[c_key])
                        return str(val).strip() if pd.notna(val) else ""
                    
                    def _is_true(c_key):
                        val = _get_val(c_key)
                        if not val:
                            return False
                        return val.upper() in ['TRUE', '1', '1.0', 'YES', 'ON']

                    sheet_name = _get_val('sheet')
                    if not sheet_name:
                        continue

                    # Activeåˆ¤å®š (é€²è»æŒ‡ç¤º)
                    # NOTE: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚‰ç„¡æ¡ä»¶ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆç®—å¯¾è±¡
                    if _is_true('active'):
                        target_sheets.append(sheet_name)
                        
                        # å‚™è€ƒåˆ—ã‹ã‚‰åˆæœŸåœ¨åº«ã‚’è‡ªå‹•ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                        note_val = _get_val('note')
                        if note_val:
                            _import_initial_from_note(xls, note_val, HISTORY_PATH)
                    
                    # Displayåˆ¤å®š (ç›£è¦–ãƒ»åºƒå ±)
                    # NOTE: è¡¨ç¤ºãƒ•ãƒ©ã‚°ONãªã‚‰Zeusã®ç›£è¦–ãƒªã‚¹ãƒˆã«å…¥ã‚Œã‚‹
                    if _is_true('display'):
                        event_info = {
                            "name": _get_val('name') or sheet_name,
                            "sheet": sheet_name,
                            "deadline": _get_val('deadline'),
                            "date": _get_val('date'),
                            "venue": _get_val('venue'),
                            "booth": _get_val('booth'),
                            "loadin": _get_val('loadin'),
                            "is_active": _is_true('active')
                        }
                        display_events.append(event_info)

                logger.info(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¤ãƒ™ãƒ³ãƒˆ (è¨ˆç®—å¯¾è±¡): {target_sheets}")
                logger.info(f"ğŸ‘€ è¡¨ç¤ºã‚¤ãƒ™ãƒ³ãƒˆ (ç›£è¦–å¯¾è±¡): {[e['name'] for e in display_events]}")
                
                # --- Zeusç›£è¦–ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ (event_master.json) ---
                event_json_path = os.path.join(DATA_DIR, 'event_master.json')
                try:
                    with open(event_json_path, 'w', encoding='utf-8') as f:
                        json.dump(display_events, f, indent=2, ensure_ascii=False)
                    logger.info(f"ç›£è¦–ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆä¿å­˜å®Œäº†: {event_json_path}")
                except Exception as e:
                    logger.error(f"ç›£è¦–ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆä¿å­˜å¤±æ•—: {e}")

            else:
                logger.warning("âš ï¸ ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿ã‹ã‚‰ 'å¯¾è±¡ã‚·ãƒ¼ãƒˆ' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

        except Exception as e:
            logger.error(f"ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        logger.warning("âš ï¸ 'ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹ã‚¿' ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    # 2. å„ã‚·ãƒ¼ãƒˆã‹ã‚‰ç›®æ¨™ã‚’åˆç®—
    aggregated_targets = {} # {clean_id: {'total': 0, 'details': []}}
    
    for sheet in target_sheets:
        if sheet not in xls.sheet_names:
            logger.warning(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ãƒˆ '{sheet}' ãŒExcelå†…ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            continue

        try:
            df_raw = pd.read_excel(xls, sheet_name=sheet, header=None)
            
            # ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã‚’æ¢ã™ (ID ã¨ã„ã†æ–‡å­—ãŒã‚ã‚‹è¡Œ)
            header_row_idx = -1
            id_col_idx = -1
            
            for r_idx in range(min(10, len(df_raw))):
                row_vals = [str(x).strip().upper() for x in df_raw.iloc[r_idx].values]
                if 'ID' in row_vals:
                    header_row_idx = r_idx
                    id_col_idx = row_vals.index('ID')
                    break
            
            if header_row_idx == -1:
                logger.warning(f"ã‚·ãƒ¼ãƒˆ '{sheet}' ã‹ã‚‰ 'ID' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
                
            target_col_idx = 5  # Fåˆ—
            current_col_idx = 6  # Gåˆ—
            
            # ãƒ‡ãƒ¼ã‚¿åå¾©
            for i in range(header_row_idx + 1, len(df_raw)):
                row = df_raw.iloc[i]
                
                if id_col_idx >= len(row): continue
                raw_id = row.iloc[id_col_idx]
                if pd.isna(raw_id): continue
                clean_id = str(raw_id).strip()
                if not clean_id: continue
                
                tgt_val = 0
                if target_col_idx < len(row):
                    try:
                        val = row.iloc[target_col_idx]
                        tgt_val = int(float(val)) if pd.notna(val) else 0
                    except:
                        tgt_val = 0
                
                cur_val = 0
                if current_col_idx < len(row):
                    try:
                        val = row.iloc[current_col_idx]
                        cur_val = int(float(val)) if pd.notna(val) else 0
                    except:
                        cur_val = 0

                if tgt_val > 0 or cur_val > 0:
                    if clean_id not in aggregated_targets:
                        aggregated_targets[clean_id] = {'target_total': 0, 'current_total': 0, 'details': []}
                    
                    aggregated_targets[clean_id]['target_total'] += tgt_val
                    aggregated_targets[clean_id]['current_total'] += cur_val
                    aggregated_targets[clean_id]['details'].append(f"{sheet}: ç›®æ¨™{tgt_val}/åœ¨åº«{cur_val}")
                    
        except Exception as e:
            logger.error(f"ã‚·ãƒ¼ãƒˆ '{sheet}' é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")

    # 3. master_list ã«åæ˜ 
    merge_count = 0
    
    # å±¥æ­´ä¿å­˜ç”¨ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆdetails ã‚’å¿…ãšå«ã‚ã‚‹ï¼‰
    history_data = {
        "timestamp": pd.Timestamp.now().isoformat(),
        "total_target": 0,
        "total_current": 0,
        "type": "scan",
        "details": {}  # â˜… å•†å“IDã”ã¨ã®å€‹æ•°ã‚’å¿…ãšå«ã‚ã‚‹
    }

    for item in master_list:
        raw_id = item.get('id', '')
        clean_id = str(raw_id).strip()
        
        target_info = aggregated_targets.get(clean_id)
        if target_info:
            t_total = target_info['target_total']
            c_total = target_info['current_total']
            details = target_info['details']
            
            item['target_quantity'] = t_total
            item['event_sheet_stock'] = c_total 
            item['remaining'] = max(0, t_total - c_total)
            
            count_for_history = c_total
            target_for_history = t_total
            
            if 'event_data' not in item:
                item['event_data'] = {}
            
            item['event_data']['åˆç®—å†…è¨³'] = ", ".join(details)
            item['event_data']['ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¤ãƒ™ãƒ³ãƒˆ'] = ", ".join(target_sheets)
            
            merge_count += 1
        else:
            item['target_quantity'] = 0
            item['remaining'] = 0
            if 'event_data' in item:
                 item['event_data'].pop('åˆç®—å†…è¨³', None)
                 item['event_data'].pop('ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¤ãƒ™ãƒ³ãƒˆ', None)
            
            count_for_history = item.get('current_stock', 0)
            target_for_history = 0

        # --- å±¥æ­´è©³ç´°ã¸è¿½åŠ  (å…¨ã‚¢ã‚¤ãƒ†ãƒ å¯¾è±¡) ---
        history_data["total_target"] += target_for_history
        history_data["total_current"] += count_for_history
        
        # â˜… å•†å“IDã”ã¨ã®å€‹æ•°ã‚’ details ã«å¿…ãšè¨˜éŒ²
        if clean_id:
            history_data['details'][clean_id] = {
                "count": count_for_history,
                "target": target_for_history
            }

    logger.info(f"å…¨ã‚¤ãƒ™ãƒ³ãƒˆåˆç®—å®Œäº†: {merge_count} ã‚¢ã‚¤ãƒ†ãƒ ã«ç›®æ¨™ã‚’è¨­å®š")
    
    # --- å±¥æ­´ã®ä¿å­˜ ---
    if not os.path.exists(HISTORY_PATH):
        history_data["type"] = "initial"
        try:
            with open(HISTORY_PATH, 'w', encoding='utf-8') as f:
                json.dump([history_data], f, indent=2, ensure_ascii=False)
            logger.info(f"å±¥æ­´åˆæœŸåŒ–: {HISTORY_PATH} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
            # --- Phase 1: DriveåŒæœŸ ---
            if upload_to_drive and HISTORY_SUMMARY_DRIVE_ID:
                _ok, _msg = upload_to_drive(HISTORY_PATH, HISTORY_SUMMARY_DRIVE_ID)
                logger.info(f"[DriveåŒæœŸ] å±¥æ­´åˆæœŸåŒ–å¾Œ: {_msg}")
        except Exception as e:
            logger.error(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {e}")
    else:
        try:
            with open(HISTORY_PATH, 'r', encoding='utf-8') as f:
                hist_list = json.load(f)
            
            hist_list.append(history_data)
            
            with open(HISTORY_PATH, 'w', encoding='utf-8') as f:
                json.dump(hist_list, f, indent=2, ensure_ascii=False)
            # --- Phase 1: DriveåŒæœŸ ---
            if upload_to_drive and HISTORY_SUMMARY_DRIVE_ID:
                _ok, _msg = upload_to_drive(HISTORY_PATH, HISTORY_SUMMARY_DRIVE_ID)
                logger.info(f"[DriveåŒæœŸ] å±¥æ­´è¿½è¨˜å¾Œ: {_msg}")
        except Exception as e:
            logger.error(f"å±¥æ­´è¿½è¨˜å¤±æ•—: {e}")

    # --- JSONæ›¸ãå‡ºã— ---
    try:
        os.makedirs(os.path.dirname(JSON_PATH), exist_ok=True)
        with open(JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(master_list, f, indent=2, ensure_ascii=False)
        msg = f"SUCCESS: Merged production_master.json saved at {JSON_PATH} ({len(master_list)} items)"
        logger.info(msg)
        print(msg)
    except Exception as e:
        logger.error(f"Merged JSONæ›¸ãå‡ºã—å¤±æ•—: {e}")
        print(f"ERROR: Failed to save merged production_master.json: {e}")

    return master_list


def import_initial_stock(excel_path=None, sheet_name='ã‚¯ãƒªãƒ2512'):
    """
    æŒ‡å®šã•ã‚ŒãŸExcelã‚·ãƒ¼ãƒˆã‹ã‚‰åˆæœŸåœ¨åº«ï¼ˆIDãƒ™ãƒ¼ã‚¹ï¼‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€
    history_summary.json ã« type="initial" ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚

    Args:
        excel_path (str): ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx ã®ãƒ‘ã‚¹ã€‚Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨ã€‚
        sheet_name (str): èª­ã¿è¾¼ã‚€ã‚·ãƒ¼ãƒˆåã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'ã‚¯ãƒªãƒ2512'ã€‚
    """
    # Excelãƒ‘ã‚¹ã®ç‰¹å®š
    if excel_path is None:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã®æ§‹ç¯‰ (ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx ã¯ CSVã®éš£ã«ã‚ã‚‹ã¯ãš)
        # CSV_PATH = .../data/ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx - å•†å“ãƒã‚¹ã‚¿.csv
        # ã‚ˆã£ã¦ data/ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx ã‚’æ¢ã™
        excel_path = os.path.join(DATA_DIR, 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼.xlsx')

    if not os.path.exists(excel_path):
        logger.error(f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ {excel_path}")
        return

    logger.info(f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹: {excel_path} (Sheet: {sheet_name})")

    try:
        # Excelèª­ã¿è¾¼ã¿
        # Cåˆ—=ID (index 2), AKåˆ—=æ®‹ (index 36), ALåˆ—=é‡‘é¡ (index 37)
        # header=0 (1è¡Œç›®) ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã™ã‚‹
        df = pd.read_excel(excel_path, sheet_name=sheet_name)
        
        # ã‚«ãƒ©ãƒ ç‰¹å®š (åå‰ã§æ¢ã™ãŒã€ä½ç½®ã‚‚è€ƒæ…®)
        # AKåˆ—ã¯37ç•ªç›®(0-indexã§36)
        
        id_col = None
        count_col = None
        value_col = None

        # 1. ã‚«ãƒ©ãƒ åã§æ¢ç´¢
        for col in df.columns:
            c_str = str(col).strip()
            if c_str == 'ID': id_col = col
            if 'æ®‹' in c_str: count_col = col # "æ®‹"ã‚’å«ã‚€ã‚«ãƒ©ãƒ 
            if 'é‡‘é¡' in c_str: value_col = col
        
        # 2. ä½ç½®ã§å¼·åˆ¶æŒ‡å®šï¼ˆæŒ‡ç¤ºå„ªå…ˆï¼‰
        # Cåˆ—=2, AKåˆ—=36, ALåˆ—=37
        # pandasã®read_excelçµæœã®columnsã®ä¸¦ã³ãŒExcelé€šã‚Šã‹ã‚ã‹ã‚‰ãªã„ãŸã‚ã€ilocã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ–¹ãŒå®‰å…¨ã‹ï¼Ÿ
        # ã—ã‹ã—ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—ã‚¢ã‚¯ã‚»ã‚¹ã¯åå‰ãŒåŸºæœ¬ã€‚
        # ã“ã“ã§ã¯æŒ‡ç¤ºé€šã‚Šã€ŒCåˆ—ã€AKåˆ—ã€ALåˆ—ã€ã‚’ä½ç½®ã§ç‰¹å®šã™ã‚‹æˆ¦ç•¥ã‚’æ¡ã‚‹ã€‚
        # ãŸã ã—ã€read_excelã®æŒ™å‹•ã«ã‚ˆã‚Šç„¡é§„ãªåˆ—ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
        # usecols ã§æŒ‡å®šã—ã¦èª­ã¿ç›´ã™ã®ãŒæœ€ã‚‚ç¢ºå®Ÿã€‚
        
        df = pd.read_excel(excel_path, sheet_name=sheet_name, usecols="C,AK,AL")
        # èª­ã¿è¾¼ã¿å¾Œã®ã‚«ãƒ©ãƒ åã¯å…ƒã®ãƒ˜ãƒƒãƒ€ãƒ¼ã«ãªã‚‹
        # 0ç•ªç›®: ID, 1ç•ªç›®: AKåˆ—ã®ãƒ˜ãƒƒãƒ€ãƒ¼, 2ç•ªç›®: ALåˆ—ã®ãƒ˜ãƒƒãƒ€ãƒ¼
        
        initial_data_details = {}
        total_value = 0
        total_count = 0
        
        for index, row in df.iterrows():
            # 1è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦æ¶ˆè²»ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ‡ãƒ¼ã‚¿ã¯2è¡Œç›®ã‹ã‚‰
            
            # ID (0ç•ªç›®)
            raw_id = row.iloc[0]
            if pd.isna(raw_id): continue
            clean_id = str(raw_id).strip()
            if not clean_id: continue
            
            # Count (1ç•ªç›®: AKåˆ—)
            raw_count = row.iloc[1]
            try:
                count = int(raw_count) if pd.notna(raw_count) else 0
            except:
                count = 0
                
            # Value (2ç•ªç›®: ALåˆ—)
            raw_value = row.iloc[2]
            try:
                value = int(raw_value) if pd.notna(raw_value) else 0
            except:
                value = 0
            
            if count > 0 or value > 0:
                initial_data_details[clean_id] = {
                    "count": count,
                    "value": value
                }
                total_value += value
                total_count += count
        
        # JSONæ§‹é€ ä½œæˆ
        # æ—¥ä»˜å›ºå®š: 2025-12-14 (ã‚¯ãƒªãƒ2512æœ€çµ‚æ—¥)
        from datetime import datetime
        date_str = "2025-12-14"
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚‚ã“ã®æ—¥ã®çµ‚ã‚ã‚Šã«è¨­å®š
        timestamp_str = datetime(2025, 12, 14, 23, 59, 59).isoformat()
        
        new_entry = {
            "type": "initial",
            "date": date_str,
            "timestamp": timestamp_str,
            "summary": total_value, # ALåˆ—åˆè¨ˆ
            "total_current": total_count,
            "details": initial_data_details,
            "total_target": 0 # åˆæœŸåœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®æ–‡è„ˆã§ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸æ˜
        }

        # history_summary.json æ›´æ–°
        history_list = []
        if os.path.exists(HISTORY_PATH):
            try:
                with open(HISTORY_PATH, 'r', encoding='utf-8') as f:
                    history_list = json.load(f)
            except:
                history_list = []
        
        # æ—¢å­˜ã® type: "initial" ã‚’å‰Šé™¤ã—ã¦å…¥ã‚Œæ›¿ãˆ
        history_list = [h for h in history_list if h.get('type') != 'initial']
        
        # å…ˆé ­ã«è¿½åŠ ï¼ˆã‚ã‚‹ã„ã¯æ™‚ç³»åˆ—é †ï¼ŸåˆæœŸåœ¨åº«ãªã®ã§å…ˆé ­ãŒè‡ªç„¶ï¼‰
        history_list.insert(0, new_entry)
        
        # ä¿å­˜
        with open(HISTORY_PATH, 'w', encoding='utf-8') as f:
            json.dump(history_list, f, indent=2, ensure_ascii=False)
            
        msg = f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {len(initial_data_details)} ä»¶, ç·æ•° {total_count}, ç·é¡ {total_value}"
        logger.info(msg)
        print(msg)
        return new_entry

    except Exception as e:
        logger.error(f"åˆæœŸåœ¨åº«ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ERROR: {e}")
        return None

if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œæ™‚ã¯å¼·åˆ¶å¤‰æ›ï¼ˆDriveåŒæœŸå«ã‚€ï¼‰
    convert_csv_to_json(force=True)
